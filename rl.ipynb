{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL?\n",
    "\n",
    "Reinforcement Learning, the third paradigm of machine learning (just because it has word 'learning' inside).\n",
    "No, just kidding. Learn from rewards.\n",
    "\n",
    "Good reference: [Reinforcement Learning: An Introduction (2nd edition, draft)](https://webdocs.cs.ualberta.ca/~sutton/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "Comes from different subject and study. Control theory, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandit Problem\n",
    "\n",
    "stateless RL. Main problem, how to balance exploration/exploitation\n",
    "\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>\n",
    "<img src=http://www.winneratslots.com/wp-content/uploads/2014/07/One-Armed-Bandit.jpg style=\"width: 91px\" align='left'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the algorithm, we create 2000 experiments with different setting of reward for 10 armed bandit, initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_testbed = 2000\n",
    "n_k = 10\n",
    "qstar = np.zeros((n_testbed, n_k))\n",
    "\n",
    "np.random.seed(0)\n",
    "for t in xrange(n_testbed):\n",
    "    for k in xrange(n_k):\n",
    "        qstar[t, k] = np.random.normal(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "samples = np.zeros((n_k, sample_size))\n",
    "for i in xrange(n_k):\n",
    "    samples[i, :] = np.random.normal(qstar[0, i],1, size=sample_size)\n",
    "    \n",
    "df = pd.DataFrame(samples.transpose())\n",
    "sns.violinplot(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon Greedy\n",
    "The most basic solution is epsilon greedy algorithm, where at each timesteps there is small probability that we take random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def egreedy_bandit(qstar, epsilon):\n",
    "    \"\"\"Implement epsilon greedy bandit\"\"\"\n",
    "    \n",
    "    k = len(qstar)\n",
    "    Q = np.zeros(k)\n",
    "    N = np.zeros(k)\n",
    "    rewards = np.zeros(1000)\n",
    "    for t in xrange(1000):\n",
    "        \n",
    "        if np.random.uniform() > epsilon:\n",
    "            a = Q.argmax()\n",
    "        else:\n",
    "            a = np.random.randint(k)\n",
    "        \n",
    "        r = np.random.normal(qstar[a], 1)\n",
    "        N[a] += 1\n",
    "        Q[a] = Q[a] + (r - Q[a])/N[a]\n",
    "        \n",
    "        rewards[t] = r\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate with different epsilon parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_rewards1 = np.zeros(1000)\n",
    "mean_rewards2 = np.zeros(1000)\n",
    "mean_rewards3 = np.zeros(1000)\n",
    "for i in xrange(n_testbed):\n",
    "    \n",
    "    mean_rewards1 += egreedy_bandit(qstar[i, :], 0)\n",
    "    mean_rewards2 += egreedy_bandit(qstar[i, :], 0.1)\n",
    "    mean_rewards3 += egreedy_bandit(qstar[i, :], 0.01)\n",
    "    \n",
    "mean_rewards1 = mean_rewards1 / n_testbed\n",
    "mean_rewards2 = mean_rewards2 / n_testbed\n",
    "mean_rewards3 = mean_rewards3 / n_testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(mean_rewards1, label='e=0')\n",
    "plt.plot(mean_rewards2, label='e=0.1')\n",
    "plt.plot(mean_rewards3, label='e=0.01')\n",
    "plt.ylim((0, 1.5))\n",
    "plt.ylabel('average reward')\n",
    "plt.xlabel('timestep')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper Confidence Bound\n",
    "In naive epsilon greedy, the action is picked randomly. However, using UCB we can pick the action based on confidence.\n",
    "The more the action has been picked, and if it returns cumulative lower rewards, then it will less likely to be picked in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ucb_bandit(qstar, c):\n",
    "    \"\"\"Upper confidence bound\"\"\"\n",
    "    \n",
    "    k = len(qstar)\n",
    "    Q = np.zeros(k)\n",
    "    N = np.ones(k)\n",
    "    rewards = np.zeros(1000)\n",
    "    for t in xrange(1000):\n",
    "        \n",
    "        At = np.array([x + c*(math.sqrt(math.log(t+1)/N[a])) for (a, x) in enumerate(Q)])\n",
    "        a = At.argmax()\n",
    "                \n",
    "        r = np.random.normal(qstar[a], 1)\n",
    "        N[a] += 1\n",
    "        Q[a] = Q[a] + (r - Q[a])/N[a]\n",
    "        \n",
    "        rewards[t] = r\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_rewards1 = np.zeros(1000)\n",
    "mean_rewards2 = np.zeros(1000)\n",
    "for i in xrange(n_testbed):\n",
    "    mean_rewards1 += egreedy_bandit(qstar[i, :], 0.1)\n",
    "    mean_rewards2 += ucb_bandit(qstar[i, :], 2)\n",
    "\n",
    "mean_rewards1 = mean_rewards1/n_testbed\n",
    "mean_rewards2 = mean_rewards2/n_testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(mean_rewards1, label='e=0.1')\n",
    "plt.plot(mean_rewards2, label='c=2')\n",
    "plt.ylim((0.2, 1.6))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Contextual multi-armed bandits\n",
    "\n",
    "Special case of multi-armed bandits where at each timestamp we are given context for the next. One example, maybe the bandits reward get shuffled.\n",
    "\n",
    "No algorithm given in the books. WTF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Problem\n",
    "\n",
    "<img src='http://i.imgur.com/c4YxfBs.png' style='width: 500px'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Environment\n",
    "\n",
    "<img src='http://i.imgur.com/voT1qCJ.png' style='width: 500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement above system, the environment at least have to have transition function and reward function.\n",
    "The agent needs to have value function and policy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dynamic Programming\n",
    "\n",
    "Refer to the solution that have exact state.\n",
    "\n",
    "Download images here: https://www.dropbox.com/s/egjn29qj87drdwo/pictures.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = []\n",
    "for i in xrange(16):\n",
    "    img = plt.imread('rl/step'+str(i+1)+'.png')\n",
    "    steps += [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 5))\n",
    "for i in xrange(16):\n",
    "    plt.subplot(1, 16, i+1)\n",
    "    plt.imshow(steps[i])\n",
    "    plt.xticks([16+i], [i])\n",
    "    plt.yticks([0], [''])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example: robot walking\n",
    "It has 16 possible states depends on the position of the legs and 4 possible actions (move right leg up or down, move right leg forward or backward, move left leg up or down, move left leg forward or backward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RobotWalkingEnvironment(object):\n",
    "    \n",
    "    def __init__(self, state):\n",
    "        \n",
    "        self.state = state\n",
    "        self.trans = ((1, 3, 4, 12),    #0\n",
    "                      (0, 2, 5, 13),    #1\n",
    "                    (3,1,6,14),\n",
    "                    (2,0,7,15),\n",
    "                    (5,7,0,8),\n",
    "                    (4,6,1,9),\n",
    "                    (7,5,2,10),\n",
    "                    (6,4,3,11),\n",
    "                    (9,11,12,4),\n",
    "                    (8,10,13,5),\n",
    "                    (11,9,14,6),\n",
    "                    (10,8,15,7),\n",
    "                    (13,15,8,0),\n",
    "                    (12,14,9,1),\n",
    "                    (15,13,10,2),\n",
    "                    (14,12,11,3)\n",
    "                    )\n",
    "        \n",
    "        r = np.zeros((16, 4))\n",
    "        r[15][1] = 1 #Moving forward by move the leg backward\n",
    "        r[0][1] = -2 #punish moving backward which is move the leg forward without lift them\n",
    "        r[0][3] = -2\n",
    "        r[3][3] = -2\n",
    "        r[12][1] = -2\n",
    "        r[2][3] = -2\n",
    "        r[8][1] = -2\n",
    "        r[1][3] = -2\n",
    "        r[4][1] = -2\n",
    "        r[1][2] = -10 # Lift one leg when other already lifted\n",
    "        r[2][2] = -10\n",
    "        r[4][0] = -10\n",
    "        r[7][0] = -10\n",
    "        r[8][0] = -10\n",
    "        r[11][0] = -10\n",
    "        r[13][2] = -10\n",
    "        r[14][2] = -10\n",
    "        self.r = r\n",
    "        \n",
    "        \n",
    "    def go(self, a):\n",
    "        \"\"\"Go to the next state given current action\"\"\"\n",
    "        reward = self.r[self.state][a]\n",
    "        self.state = self.trans[self.state][a]\n",
    "        \n",
    "        return reward, self.state\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "        \n",
    "class QAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.Q = np.random.random((16, 4))\n",
    "        self.eta = 0.5                   # learning rate\n",
    "        self.gamma = 0.8                 # discount factor\n",
    "        self.epsilon = 0.1\n",
    "    \n",
    "    def get_action(self, state, train=False):\n",
    "        if train and np.random.uniform() < self.epsilon:\n",
    "            return int(np.random.uniform()*4)\n",
    "        return np.argmax(self.Q[state, :])\n",
    "    \n",
    "    def update(self, cur_state, action, new_state, reward):\n",
    "        self.Q[cur_state, action] += self.eta*(reward + self.gamma*max(self.Q[new_state, :]) - self.Q[cur_state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "Solved using Bellman Equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = RobotWalkingEnvironment(0)\n",
    "\n",
    "# The state value function\n",
    "V = np.zeros(16)\n",
    "gamma = 0.8\n",
    "\n",
    "# The policy function which map state to action, randomly given value at the beginning\n",
    "pi = np.array([int(np.random.random()*4) for i in range(16)])\n",
    "\n",
    "for p in range(100):\n",
    "    for s in range(len(pi)):\n",
    "        pi[s] = np.argmax([env.r[s][a] + gamma*V[env.trans[s][a]] for a in xrange(4)])\n",
    "\n",
    "        for s in range(len(V)):\n",
    "            a = pi[s]\n",
    "            V[s] = env.r[s][a] + gamma*V[env.trans[s][a]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = RobotWalkingEnvironment(0)\n",
    "state = 0\n",
    "sequence = [0]\n",
    "for i in xrange(100):\n",
    "    nextState = env.trans[state][pi[state]]\n",
    "    sequence = sequence + [nextState]\n",
    "    \n",
    "    plt.imshow(steps[state])\n",
    "    plt.axis('off')\n",
    "    plt.gcf().canvas.draw()\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    state = nextState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning\n",
    "\n",
    "An instance of Temporal Difference Learing in case where we do not have access to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = QAgent()\n",
    "\n",
    "for i in xrange(1):\n",
    "    state = 0\n",
    "    env = RobotWalkingEnvironment(state)\n",
    "    for t in xrange(1000):\n",
    "        a = agent.get_action(state, train=True)\n",
    "\n",
    "        reward, new_state = env.go(a)\n",
    "        agent.update(state, a, new_state, reward)\n",
    "        \n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simulate optimal strategy\n",
    "\n",
    "states = [0]\n",
    "state = 0\n",
    "count = 0\n",
    "while count <= 100:\n",
    "    \n",
    "    a = agent.get_action(state)\n",
    "    reward, new_state = env.go(a)\n",
    "    state = new_state\n",
    "    \n",
    "    states += [state]    \n",
    "    count += 1\n",
    "    \n",
    "    plt.imshow(steps[state])\n",
    "    plt.axis('off')\n",
    "    plt.gcf().canvas.draw()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI gym\n",
    "\n",
    "It is a platform to compare multiple reinforcement learning algorithm. Providing diverse sets of problem to be solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for t in range(1000):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "    print observation\n",
    "    if done:\n",
    "        print 'done'\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print env.action_space\n",
    "print env.observation_space\n",
    "\n",
    "print env.observation_space.high\n",
    "print env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "for i_episode in xrange(10):\n",
    "    observation = env.reset()\n",
    "    for t in xrange(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print 'Episode {} finished after {} timesteps'.format(i_episode+1, t+1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1', force=True)\n",
    "\n",
    "for i_episode in xrange(10):\n",
    "    observation = env.reset()\n",
    "    for t in xrange(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        _, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            print 'done'\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gym.upload('/tmp/cartpole-experiment-1', api_key='sk_RIprDTFeQoWlT0enGNPxg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise, implements Q-learning to solve CartPole problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
